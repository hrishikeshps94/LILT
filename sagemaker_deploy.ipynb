{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Code for deploying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inference.py\n",
    "from utils import visualize_predictions, aggregate_outputs\n",
    "import onnxruntime\n",
    "import os\n",
    "from transformers import LayoutLMv3TokenizerFast\n",
    "from local_infer import predict_page\n",
    "from config import logger\n",
    "import json\n",
    "import torch\n",
    "from model import TokenClassificationModel\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer for inference.\n",
    "\n",
    "    Args:\n",
    "        model_dir (str): The directory path where the model is stored.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the loaded model and tokenizer.\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    load_path = os.path.join(model_dir, \"quantized_best.onnx\")\n",
    "    model = onnxruntime.InferenceSession(\n",
    "        load_path, providers=[\"CPUExecutionProvider\"])\n",
    "    # If you want to load the pytorch model\n",
    "    # logger.info(f\"Loading model from {model_dir}\")\n",
    "    # load_path = os.path.join(model_dir, \"best.pt\")\n",
    "    # weights = torch.load(load_path, map_location=\"cpu\")\n",
    "    # model = TokenClassificationModel(weights['config'])\n",
    "    # trained_state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k,\n",
    "    #                       v in weights['model'].items()}\n",
    "    # model.load_state_dict(trained_state_dict)\n",
    "    tokenizer = LayoutLMv3TokenizerFast.from_pretrained(\n",
    "        os.path.join(model_dir, \"tokenizer\"))\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    logger.info(\n",
    "        f\"Request content type: {request_content_type}, received request body: {request_body}\")\n",
    "    if request_content_type == 'application/json':\n",
    "        return json.loads(request_body)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'Content type must be application/json. Provided: {0}'.format(request_content_type))\n",
    "\n",
    "\n",
    "def predict_fn(data, model):\n",
    "    \"\"\"\n",
    "    Make predictions on the input data.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The input data. List of dictionaries containing the text and bbox.\n",
    "        model (tuple): The loaded model and tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        dict: The predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(f\"Processing started\")\n",
    "    # Predict for each page\n",
    "    # Model , data, tokenizer\n",
    "    predictions = predict_page(model[0], data, model[1])\n",
    "    logger.info(f\"Processing Completed , Predictions: {predictions}\")\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    logger.info(f\"Output data of type {accept} returned: {prediction}\")\n",
    "    if accept == 'application/json':\n",
    "        return json.dumps({'generated_text': prediction})\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Unsupported content type: {}\".format(accept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "transformers\n",
    "onnx\n",
    "onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zipping Model and Inference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm model.tar.gz\n",
    "!rm -rf deploy\n",
    "!rm -rf .ipynb_checkpoints*\n",
    "os.makedirs(\"deploy/code\", exist_ok=True)\n",
    "!cp inference.py local_infer.py _inference.py config.py utils.py model.py requirements.txt deploy/code\n",
    "!cp -r weights/best.pt weights/tokenizer weights/quantized_best.onnx deploy/\n",
    "!cd deploy && tar -czvf ../model.tar.gz *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the model in AWS Sagemaker\n",
    "- Pre-requisites:\n",
    "    - AWS Account\n",
    "    - Sagemaker Domain\n",
    "    - IAM Role with Sagemaker Full Access\n",
    "    - AWS CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "# Create a SageMaker session\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# Initialize a variable for the SageMaker session bucket\n",
    "# Replace None with the name of the SageMaker session bucket\n",
    "sagemaker_session_bucket = None\n",
    "\n",
    "# If the SageMaker session bucket is not defined, set it to the default bucket\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "# Try to get the execution role for the SageMaker session\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    # If the execution role cannot be retrieved, manually define it using boto3\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"SageMakerExecutionRole\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "# Create a new SageMaker session with the default bucket set to the SageMaker session bucket\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "# Get the region name for the SageMaker session\n",
    "region = sess.boto_region_name\n",
    "\n",
    "# Print the execution role, default bucket, and region for the SageMaker session\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_prefix = 'lilt-layout'\n",
    "s3_model_key = f'{generation_prefix}/model/model.tar.gz'\n",
    "s3_model_location = f\"s3://{sagemaker_session_bucket}/{s3_model_key}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional whenever there is a need to deploy the updated model in AWS Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\"s3\")\n",
    "s3.Bucket(sagemaker_session_bucket).upload_file(\"model.tar.gz\", s3_model_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serverless deployment steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_name = f\"{generation_prefix}-deployment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.serverless.serverless_inference_config import ServerlessInferenceConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serverless_config = ServerlessInferenceConfig(\n",
    "    memory_size_in_mb=3072,  # Till 6GB is possible\n",
    "    max_concurrency=1, # Can be increased\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = image_uris.retrieve(\n",
    "    framework='pytorch',\n",
    "    region=region,\n",
    "    version='2.1.0',\n",
    "    image_scope='inference',\n",
    "                serverless_inference_config=serverless_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model = PyTorchModel(\n",
    "    name=deployment_name,\n",
    "    py_version='py310',\n",
    "    framework_version='2.1.0',\n",
    "    model_data=s3_model_location,\n",
    "    entry_point='inference.py',\n",
    "    role=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictor = pytorch_model.deploy(\n",
    "    serverless_inference_config=serverless_config,\n",
    "    endpoint_name=deployment_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import Predictor\n",
    "predictor = Predictor(endpoint_name=deployment_name, sagemaker_session=sess,\n",
    "                      serializer=JSONSerializer(), deserializer=JSONDeserializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction using the deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"path_to_pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_processor import extract_text_and_bbox_from_pdf\n",
    "from utils import visualize_predictions\n",
    "file_name = os.path.basename(pdf_path).split(\".\")[0]\n",
    "data = extract_text_and_bbox_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for page_num, page in enumerate(data):\n",
    "    img_cv = page[\"image\"]\n",
    "    text_data = page[\"text\"]\n",
    "    page_prediction = predictor.predict(text_data)\n",
    "    visualize_predictions(img_cv, page_prediction['generated_text'],\n",
    "                          f'{file_name}-{page_num}')\n",
    "    predictions.append(page_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
